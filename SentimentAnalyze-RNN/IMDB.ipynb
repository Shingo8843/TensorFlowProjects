{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1de2e93-01f3-4ec2-92a5-9e520a8eaa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbe0b855-3c10-4baf-84cf-36125ab8d5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "# Load the IMDB dataset\n",
    "max_words = 10000  # Vocabulary size\n",
    "max_len = 500  # Maximum review length\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_words)\n",
    "\n",
    "# Pad sequences to ensure uniform input size\n",
    "x_train = pad_sequences(x_train, maxlen=max_len)\n",
    "x_test = pad_sequences(x_test, maxlen=max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8869a25c-2110-45c5-aadd-6937a22b25da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shing\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# define RNN\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=max_words, output_dim=128, input_length=max_len),\n",
    "    LSTM(64, return_sequences=False),  # You can use GRU or simple RNN here\n",
    "    Dense(1, activation='sigmoid')  # Binary classification\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1b5136e-b34b-4215-909d-7d105ad43d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 129ms/step - accuracy: 0.7071 - loss: 0.5398 - val_accuracy: 0.8742 - val_loss: 0.3197\n",
      "Epoch 2/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 128ms/step - accuracy: 0.9053 - loss: 0.2498 - val_accuracy: 0.8656 - val_loss: 0.3220\n",
      "Epoch 3/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 128ms/step - accuracy: 0.9202 - loss: 0.2116 - val_accuracy: 0.8592 - val_loss: 0.3643\n",
      "Epoch 4/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 129ms/step - accuracy: 0.9407 - loss: 0.1606 - val_accuracy: 0.8716 - val_loss: 0.3500\n",
      "Epoch 5/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 132ms/step - accuracy: 0.9506 - loss: 0.1337 - val_accuracy: 0.8658 - val_loss: 0.3984\n"
     ]
    }
   ],
   "source": [
    "# Train Model\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=5,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2  # Use 20% of training data for validation\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2f0e31d-57b4-465d-9812-7b9ad1585291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 17ms/step - accuracy: 0.8575 - loss: 0.4298\n",
      "Test Accuracy: 0.8578799962997437\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b2b3cf2-9b33-4a91-810d-7a1c133ea586",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "word_index = {k: (v + 3) for k, v in word_index.items()}\n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2\n",
    "word_index[\"<UNUSED>\"] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d6ebb4c-a441-459a-9e30-b7445733781e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(model, review, word_index, max_len=500):\n",
    "    \"\"\"\n",
    "    Predict the sentiment score of a given review using a trained model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained sentiment analysis model.\n",
    "        review: A string containing the review text.\n",
    "        word_index: Dictionary mapping words to integer indices (IMDB word index).\n",
    "        max_len: The maximum length for padding/truncating the review.\n",
    "        \n",
    "    Returns:\n",
    "        score: Sentiment score (float) between 0 and 1.\n",
    "    \"\"\"\n",
    "    # Tokenize the review\n",
    "    def tokenize_review(review, word_index):\n",
    "        tokens = []\n",
    "        for word in review.lower().split():\n",
    "            word = word.strip(\"!?.\")\n",
    "            tokens.append(word_index.get(word, word_index[\"<UNK>\"]))\n",
    "        return tokens\n",
    "\n",
    "    # Tokenize and pad the review\n",
    "    tokenized_review = tokenize_review(review, word_index)\n",
    "    padded_review = pad_sequences([tokenized_review], maxlen=max_len, padding='post')\n",
    "    \n",
    "    # Predict the sentiment score\n",
    "    score = model.predict(padded_review)[0][0]\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6e40d7e-99e6-494a-a348-f362cbcbbbba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n"
     ]
    }
   ],
   "source": [
    "sample_review = \"This movie was fantastic! The acting was great and the story was compelling.\"\n",
    "score = predict_sentiment(model, sample_review, word_index, max_len=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53b2300d-0a63-4617-916a-a65be0ab31ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive 0.81742954\n"
     ]
    }
   ],
   "source": [
    "if score > 0.5:\n",
    "    print(\"Positive\", score)\n",
    "else:\n",
    "    print(\"Negative\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd95b23a-0046-4b93-903f-fa6739343190",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
